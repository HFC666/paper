---
title: 无参贝叶斯
categories: 论文阅读
tags: [机器学习, 无参贝叶斯]
update: 2022-7-11
state: 完成
---
## A Tutorial on Bayesian Nonparametric Models
> 文章地址：https://www.sciencedirect.com/science/article/abs/pii/S002224961100071X

### Introduction

统计模型的一个核心问题就是模型选择，怎样选择模型复杂度，如在聚类问题中选择聚类的数目等。贝叶斯无参模型通过让数据确定模型复杂度来跳过模型选择的步骤。比如说在传统的机器学习中如聚类中，我们需要事先确定聚类的数目，而贝叶斯无参模型会估计聚类的数目并允许未来的数据来自之前没有的类。

一般我们进行贝叶斯推断时是对参数的后验分布进行推断，无参贝叶斯与其他贝叶斯模型显著的不同在于其隐藏结构是随着数据增长的。它的复杂度，如聚类的数目，是后验分布的一部分。这些参数无需实现给定，而是作为分析数据的一部分来确定。

在本文中我们主要介绍两种模型，混合模型(mixture models)和隐语义模型(latent factor models)。贝叶斯无参混合模型，也被称为中国餐馆过程混合(或者狄利克雷过程混合)，从数据中推断聚类的个数并且随着数据的增长允许聚类数增长。

隐语义模型将观察到的数据分解为潜在因素的线性组合。关于因子分布的不同假设会产生如因子分析、主成分分析、独立成分分析等等。印度自助餐过程隐语义模型通过数据推断因子的数量并且允许因子的数量随着新数据的加入而增加。

### Mixture models and clustering

在一个混合模型中，每一个观测数据都假设属于一个类。

#### Finite mixture modeling

有限混合模型假设有$K$个类，每个类与参数$\theta_k$有关。每一个观测$y_n$都假设是首先通过$P(c_n)$来选择一个类$c_n$之后通过与参数$\theta_{c_n}$相对应的分布中产生观测，每一个参数对应一个分布$F(y_n\mid \theta_{c_n})$。

贝叶斯混合模型引入了混合分布$P(c)$的先验和聚类参数的先验$\theta\sim G_0$。

这个生成过程定义了关于观测、聚类分配(数据属于哪个类)和聚类参数的联合分布：
$$
P(\mathrm{y,c},\theta) = \prod_{k=1}^K G_0(\theta_k)\prod_{n=1}^NF(Y_n\mid\theta_{c_n})P(c_n)
$$
其中观测为$\mathrm{y} = \{y_1,\cdots,y_N\}$，聚类分配为$\mathrm{c}=\{c_1,\cdots,c_N\}$，聚类参数为$\theta=\{\theta_1,\cdots,\theta_K\}$。

> 这里$N$个样本相乘是因为样本之间是独立同分布的，而$K$个$G_0(\theta_k)$相乘是因为每个$\theta_k$也是独立的。

给定数据集，我们经常对聚类分配感兴趣，我们可以用贝叶斯公式计算它们：
$$
P(\mathrm{c}\mid\mathrm{y}) = \frac{P(\mathrm{y}\mid \mathrm{c})P(\mathrm{c})}{\sum_{\mathrm{c}}P(\mathrm{y}\mid\mathrm{c})P(\mathrm{c})}
$$
其中似然函数可以通过对$\theta$积分得到：
$$
P(\mathrm{y\mid c}) = \int_{\theta}\left[\prod_{n=1}^NF(\mathrm{y}\mid\theta_{c_n})\prod_{k=1}^KG_0(\theta_k)\right]d\theta
$$
后验分布是很难处理的，因为计算分母需要将每种分配方式进行求和。

#### The Chinese restaurant process

见我的一篇博客(https://hfcouc.work/2022/07/06/BNP/)。

### Latent factor models and dimensionality reduction

混合模型假设每个数据都来自$K$个类中的一个。隐语义模型削弱了这一假设：每个观测受到$K$个分量的不同的影响。

隐语义模型可以用来降维当分量的数目小于数据维度时。每个观察都与一个分量激活向量（潜在因子）相关联，该向量描述了每个分量对其贡献的程度，并且这个向量可以看作是观察本身的低维表示。

非常著名的降维模型有因子分析(FA)，主成分分析(PCA)和独立成分分析(ICA)等，所有的都假设因子的数量$K$已经确定了。无参贝叶斯模型允许分量的数量随着数据的增长而增多。

在经典的降维模型中，我们的观测数据一般为$N$个$M$维向量，$\mathrm{Y=\{y_1,\cdots,y_N\}}$。因此$\mathrm{Y}$每行表示一个数据。数据假设是从有噪声的隐语义的加权组合产生的：
$$
\mathrm{y_n = Gx_n+\epsilon_n}
$$
其中$\mathrm{G}$为$M\times K$的矩阵表示，表示隐语义$k$怎么影响观测维度$m$，$x_n$为一个$K$维向量表示每个隐语义的影响，其中$\epsilon_n$维独立高斯噪声向量。我们可以将其拓展为稀疏模型通过将$\mathrm{G}$进行分解，$\mathrm{G}_{mk}=z_{mk}w_{mk}$，其中$z_{mk}$是一个二元掩码(mask)变量指示因子$k$是否影响维度$m$，而$w_{ik}$为连续权重变量。这个有时被称为`spike and slab`模型因为$x_{mk}$的期望为一个在隐语义空间上的`slab`$P(w_{mk})$和在零的位置$P(z_{mk}=0)$的`spike`。

我们使用贝叶斯的方法推理隐语义、掩码变量和权重。我们在它们上面定义先验并利用贝叶斯公式计算后验$P(\mathrm{G,Z,W\mid Y})$。

利用无参贝叶斯模型，我们令数据自动确定隐语义的数目$K$。$Z$为一个二元矩阵，有有限多的行和无限多的列。

与中国餐馆过程类似，$\mathrm{Z}$上的无限容量的分布也被赋予了一个有关烹饪的隐喻，被称为印度自助餐过程。一个顾客(维度)进入一个具有无限菜品的自主餐馆。顾客$m$选择菜品$k$的概率正比于他被之前顾客选择的次数$h_k$。当顾客考虑了所有之前采样过的菜肴($h_k>0$)的菜肴时，他会额外采样之前从未采样过的$\operatorname{Poisson}(\alpha/N)$菜肴。当所有$M$个顾客都选完菜后，形成的二元矩阵$\mathrm{Z}$从IBP(Indian buffet process)中采样得到。

与中国餐馆过程不同的是，中国餐馆过程每个观测只能选择一个隐语义，而印度自助餐过程每个观测的每个维度可以选择无数多个隐语义。如下图：

![](https://raw.githubusercontent.com/HFC666/image/master/img/IBP.jpg)

> 印度餐馆过程：IBP的生成过程，其中有序号的菱形代表顾客，与与它们相关的观测连接。大圆形表示菜品，与与它们相关的参数$\phi$相连。每个顾客选择多个菜品，并且每个顾客的观测为选择的菜品的参数的线性组合。

现在回到我们的后验分布，我们想要对我们的隐语义矩阵进行推理：
$$
P(\mathrm{X,W,Z}\mid \mathrm{Y})\propto P(\mathrm{Y\mid X,W,Z})P(\mathrm{X})P(\mathrm{W})P(\mathrm{Z})
$$
精确的推理是很困难的因为归一化参数需要对所有可能的二元矩阵进行积分。
### Inference
在BNP建模中最基本的计算问题是计算后验概率。下面我们只简单介绍几种计算方法。
+ MCMC
+ 变分贝叶斯推断：见我的一篇博客(https://hfcouc.work/2022/07/10/VBI/)

### Limitations and extensions
我们前面介绍到的两种BNP只是最简单的两种，它们存在着很多限制，为了解决这些限制有很多上述模型的拓展。
#### Hierarchical structure
第一个限制是关于分组数据的：我们如何捕捉一个群体中的共性和特性呢？比如说同一种动物既有共性也有自己的特性。解决这个问题的标准贝叶斯方法是基于层次模型，在这个模型中，个体由于来自同一群体的分布而被耦合在一起。在非参数的设定下发展出的模型为分层狄利克雷过程。
#### Time series models
第二个限制是关于序列数据的：我们怎样才能捕捉到序列观察结果之间的依赖关系？关于这方面非常著名的是隐马尔可夫模型。无限隐马尔科夫模型提出了相同的序列结构，但采用了无限多的潜在类，无限隐马尔可夫模型为分层狄利克雷过程的一个特殊情况。
作为`HMM`（隐状态为离散的）的替代品，线性动态系统（也被称为自回归移动平均模型）假定隐状态是连续的，并根据线性高斯马尔可夫过程随时间演变。在一个切换的线性动态系统中，系统可以有许多动态模式；这使得边际转移分布可以是非线性的。有学者探索了切换线性动态系统的非参数变体，其中动态模式的数量是利用HDP先验从数据中推断出来的。
#### Spatial models
很多数据集上的依赖是在空间上的。如某种疾病在某个地方出现，在它附近的地方也可能出现。在BNP模型中捕获这种依赖关系的一种方法是使DP的基分布依赖于一个位置变量。
